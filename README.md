# Machine Learning Projects

This repository contains my implementations and explorations of machine learning concepts from **Introduction to Machine Learning**. Each project focuses on different algorithms and techniques used in the field, with both coding and theoretical components.

## Table of Contents
1. [Linear Regression and Overfitting](#linear-regression-and-overfitting)
2. [K-Nearest Neighbors and Polynomial Regression](#k-nearest-neighbors-and-polynomial-regression)
3. [Perceptron, Neural Networks, and Regularization](#perceptron-neural-networks-and-regularization)
4. [Naive Bayes and the EM Algorithm](#naive-bayes-and-the-em-algorithm)
5. [EM Algorithm and Advanced Topics](#em-algorithm-and-advanced-topics)

---

## Linear Regression and Overfitting
**Objective:**  
This project was centered around building linear regression models and understanding the effects of overfitting, especially in models that use too many features or high degrees of polynomial terms.

**What I did:**  
- Implemented a linear regression model using gradient descent.
- Explored overfitting by varying the complexity of the model and number of features.
- Applied regularization techniques like L2 regularization (Ridge regression) to mitigate overfitting and improve generalization on unseen data.

---

## K-Nearest Neighbors and Polynomial Regression
**Objective:**  
This project involved implementing the k-Nearest Neighbors (k-NN) algorithm and polynomial regression to understand both distance-based learning and modeling non-linear data.

**What I did:**  
- Developed a k-NN classifier and experimented with different distance metrics to improve its performance.
- Implemented a polynomial regression model to capture non-linear patterns in data.
- Evaluated the models using metrics such as accuracy for classification and Mean Squared Error (MSE) for regression.

---

## Perceptron, Neural Networks, and Regularization
**Objective:**  
This project introduced neural networks, starting from perceptrons, and explored techniques to avoid overfitting through regularization. I worked on building simple neural networks and explored concepts such as gradient descent and backpropagation.

**What I did:**  
- Implemented a perceptron model for binary classification.
- Extended the work to build a basic multi-layer neural network.
- Used regularization methods such as L2 (weight decay) to improve the model's generalization.
- Analyzed model performance using metrics like accuracy and loss.

---

## Naive Bayes and the EM Algorithm
**Objective:**  
This project involved implementing both supervised and semi-supervised Naive Bayes classifiers. Additionally, I worked with Expectation-Maximization (EM) algorithms for unsupervised learning tasks.

**What I did:**  
- Built a fully-supervised Naive Bayes classifier to classify data.
- Implemented a semi-supervised Naive Bayes classifier using the EM algorithm to work with partially labeled data.
- Focused on optimizing the performance of the Naive Bayes model and ensured stability in calculations using log-sum-exp techniques.

---

## EM Algorithm and Advanced Topics
**Objective:**  
In this project, I continued exploring the Expectation-Maximization (EM) algorithm and delved into advanced techniques for handling sparse matrices. This project combined both theory and implementation of machine learning models that deal with missing data or sparse inputs.

**What I did:**  
- Solved practice problems using sparse matrices to handle large-scale data efficiently.
- Implemented stable softmax and log-sum functions to prevent numerical instability in large computations.
- Optimized semi-supervised learning algorithms with EM to handle data where labels are partially missing.

---

Feel free to clone this repository and explore the individual projects. Each directory contains code and explanations for the respective projects, along with test cases and instructions for running the models.

---
